# Xander Davies

<img src="https://github.com/xanderdavies/xanderdavies.github.io/assets/55059966/ae0b18a4-caad-4071-8d1a-e64f32655714" alt="xander" width="200"/>

Hi, I'm Xander. I'm a member of the technical staff at the [UK AI Safety Institute](https://x.com/alxndrdavies/status/1720435535855149513?s=20), where I work on AI safety and security. I previously studied computer science at Harvard, where I founded and led the [Harvard AI Safety Team](https://harvardaist.org), a student group aimed at supporting students in conducting research to reduce risks from advanced AI. I've also worked with [David Krueger](https://www.davidscottkrueger.com/)'s lab at Cambridge University, [David Bau](https://baulab.info/)'s lab at Northeastern University, and [Redwood Research](www.redwoodresearch.org). I enjoy writing, chess, listening to music, [and](https://drive.google.com/file/d/1a9ItWvJHRpqune1srF5lVXOg2osX_imA/view?usp=sharing) [playing](https://drive.google.com/file/d/1FPIZnW3uex4eCUomlKBqNMdyqf958JVi/view?usp=sharing) [piano](https://drive.google.com/file/d/1VRXvsDpkhYVeTdmUOT2_Lwfewkui3c_0/view?usp=sharing)[.](https://drive.google.com/file/d/1_RGtxt5Vn9Ob8-DvfG3AxNICyObTnwqf/view?usp=sharing) If you'd like to discuss any of this, feel free to email me at alexander_davies [at] college [dot] harvard [dot] edu.

## Publications
_[Google Scholar](https://scholar.google.com/citations?user=69geBIoAAAAJ)_

_\* = Equal Contribution._
  
Casper, S.\*, **Davies, X.\***, Shi, C., Gilbert, T.K., Scheurer, J., Rando, J., Freedman, R., Korbak, T., Lindner, D., Freire, P., Wang, T., Marks, S., Segerie, C.-R., Carroll, M., Peng, A., Christoffersen, P., Damani, M., Slocum, S., Anwar, U., Siththaranjan, A., Nadeau, M., Michaud, E.J., Pfau, J., Krasheninnikov, D., Chen, X., Langosco, L., Hase, P., Bıyık, E., Dragan, A., Krueger, D., Sadigh, D., Hadfield-Menell, D. (2023). [Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2307.15217). arXiv preprint arXiv:2307.15217.

**Davies, X.\***, Nadeau, M.\*, Prakash, N.\*, Shaham, T, & Bau, D. (2023). [Discovering Variable Binding Circuitry with Desiderata](https://arxiv.org/abs/2307.03637). In *2023 ICML Workshop on Deployment Challenges for Generative AI.*

Li, M.\*, **Davies, X.**\*, & Nadeau, M.\* (2023). [Circuit Breaking: Removing Model Behaviors with Targeted Ablation](https://openreview.net/forum?id=ytYaiSQNCB). In *2023 ICML Workshop on Deployment Challenges for Generative AI.*

**Davies, X.\***, Langosco, L.\*, & Krueger, D. (2022). [Unifying Grokking and Double Descent](https://arxiv.org/abs/2303.06173). In *2022 NeurIPS ML Safety Workshop.*

Bricken, T., **Davies, X.**, Singh, D., Krotov, D., & Kreiman, G. (2023). [Sparse Distributed Memory is a Continual Learner](https://arxiv.org/abs/2303.11934). In *ICLR 2023.*


## Writing & Press

[Harvard Tech Review: Alexander Davies – 2023 Top Ten Seniors In Innovation](https://harvardtechnologyreview.com/2023/10/08/alexander-davies-2023-top-ten-seniors-in-innovation/)

[Harvard Crimson: Undergraduates Ramp Up Harvard AI Safety Team Amid Concerns Over Increasingly Powerful AI Models](https://www.thecrimson.com/article/2023/3/22/haist-ai-safety/)

[Update on Harvard AI Safety Team and MIT AI Alignment](https://www.lesswrong.com/posts/LShJtvwDf4AMo992L#)

[Toy Grokking with a Linear Model](writing/toy_grok/toy_grok.html)

[Gradient Descent's Implicit Bias on Separable Data](writing/implicit_bias_sgd/gd_imp_sep.html)

[Announcing the Harvard AI Safety Team](https://forum.effectivealtruism.org/posts/NvzeAtoynxGjDnWkp/announcing-the-harvard-ai-safety-team)
