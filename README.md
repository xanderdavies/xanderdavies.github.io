# Xander Davies

<img src="https://github.com/xanderdavies/xanderdavies.github.io/assets/55059966/ae0b18a4-caad-4071-8d1a-e64f32655714" alt="xander" width="200"/>

Hi, I'm Xander. I've just graduated Harvard, where I studied computer science. During my time there, I founded and led the [Harvard AI Safety Team](https://harvardaist.org), a student group aimed at supporting students in conducting research to reduce risks from advanced AI. I previously co-led Redwood Research's [REMIX interpretability residency](https://www.redwoodresearch.org/remix), a 40-person, 5-week research effort on mechanistic interpretability. I've researched grokking and double descent with [David Krueger](https://www.davidscottkrueger.com/)'s lab at Cambridge University, and mechanistic interpretability with [David Bau](https://baulab.info/) at Northeastern University. I'm interested in making sure advanced AI is developed safely. I also enjoy writing, chess, listening to music, [and](https://drive.google.com/file/d/1a9ItWvJHRpqune1srF5lVXOg2osX_imA/view?usp=sharing) [playing](https://drive.google.com/file/d/1FPIZnW3uex4eCUomlKBqNMdyqf958JVi/view?usp=sharing) [piano](https://drive.google.com/file/d/1VRXvsDpkhYVeTdmUOT2_Lwfewkui3c_0/view?usp=sharing)[.](https://drive.google.com/file/d/1_RGtxt5Vn9Ob8-DvfG3AxNICyObTnwqf/view?usp=sharing) If you'd like to discuss any of this, feel free to email me at alexander_davies [at] college [dot] harvard [dot] edu.

## Publications

**Davies, X.\***, Nadeau, M.\*, Prakash, N.\*, Shaham, T, & Bau, D. (2023). [Discovering Variable Binding Circuitry with Desiderata]([https://openreview.net/forum?id=uoqOpOIp34](https://arxiv.org/abs/2307.03637)). In *2023 ICML Workshop on Deployment Challenges for Generative AI.*

Li, M.\*, **Davies, X.**\*, & Nadeau, M.\* (2023). [Circuit Breaking: Removing Model Behaviors with Targeted Ablation](https://openreview.net/forum?id=ytYaiSQNCB). In *2023 ICML Workshop on Deployment Challenges for Generative AI.*

**Davies, X.\***, Langosco, L.\*, & Krueger, D. (2022). [Unifying Grokking and Double Descent](https://arxiv.org/abs/2303.06173). In *2022 NeurIPS ML Safety Workshop.*

Bricken, T., **Davies, X.**, Singh, D., Krotov, D., & Kreiman, G. (2023). [Sparse Distributed Memory is a Continual Learner](https://arxiv.org/abs/2303.11934). In *ICLR 2023.*


## Writing & Press

[Harvard Crimson: Undergraduates Ramp Up Harvard AI Safety Team Amid Concerns Over Increasingly Powerful AI Models](https://www.thecrimson.com/article/2023/3/22/haist-ai-safety/)

[Update on Harvard AI Safety Team and MIT AI Alignment](https://www.lesswrong.com/posts/LShJtvwDf4AMo992L#)

[Toy Grokking with a Linear Model](writing/toy_grok/toy_grok.html)

[Gradient Descent's Implicit Bias on Separable Data](writing/implicit_bias_sgd/gd_imp_sep.html)

[Announcing the Harvard AI Safety Team](https://forum.effectivealtruism.org/posts/NvzeAtoynxGjDnWkp/announcing-the-harvard-ai-safety-team)
